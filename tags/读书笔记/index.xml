<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>读书笔记 on codedump的网络日志</title>
    <link>https://www.codedump.info/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</link>
    <description>Recent content in 读书笔记 on codedump的网络日志</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Thu, 18 Apr 2019 08:40:34 +0800</lastBuildDate><atom:link href="https://www.codedump.info/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>《数据密集型应用系统设计》第九章《一致性与共识》笔记</title>
      <link>https://www.codedump.info/post/20190406-ddia-chapter09-consistency-and-consensus/</link>
      <pubDate>Thu, 18 Apr 2019 08:40:34 +0800</pubDate>
      
      <guid>https://www.codedump.info/post/20190406-ddia-chapter09-consistency-and-consensus/</guid>
      
      <description>&lt;h1 id=&#34;一致性保证&#34;&gt;一致性保证&lt;/h1&gt;
&lt;p&gt;最终一致性（eventual consistency）：如果停止更新数据，等待一段时间（时间长度未知），则最终所有读请求将返回相同的内容。&lt;/p&gt;
&lt;p&gt;然而最终一致性是一种非常弱的一致性保证，因为无法知道何时（when）系统会收敛。而在收敛之前，读请求都可能返回任何值。&lt;/p&gt;
&lt;h1 id=&#34;可线性化linearizability&#34;&gt;可线性化（Linearizability）&lt;/h1&gt;
&lt;p&gt;可线性化（Lineariazability），也被称为原子一致性（atomic consistency）、强一致性（strong consistency），其基本的思想是让一个系统看起来好像只有一个数据副本，且所有的操作都是原子的。有了这个保证，应用程序不需要再关系系统内部有多少个副本。&lt;/p&gt;
&lt;p&gt;在一个可线性化的系统中，一旦客户端成功提交写请求，所有客户端的读请求一定能看到刚刚写入的值。这一保证让客户端认为只有一个副本，这样任何一次读取都能读到最新的值，而不是过期的数据。&lt;/p&gt;
&lt;p&gt;下图来解释在一个非线性化的系统中，可能出现什么问题。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.codedump.info/media/imgs/20190406-ddia-chapter09-consistency-and-consensus/9-1.jpg&#34; alt=&#34;9-1&#34; title=&#34;9-1&#34;&gt;&lt;/p&gt;
&lt;p&gt;上图中，alice和bob同时等待2014年世界杯决赛的结果。在宣布最终比分之后，alice看到了最终的结果，然后将此结果告诉了bob，bob马上在自己的手机上刷新想看最新的结果，但是却返回了过期的数据，显示当前比赛还在进行中。&lt;/p&gt;
&lt;h2 id=&#34;如何实现可线性化&#34;&gt;如何实现可线性化？&lt;/h2&gt;
&lt;p&gt;前面只是简单介绍了可线性化的思想：使系统看起来只有一个数据副本。为了更好的理解可线性化，看下面的图示例子。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.codedump.info/media/imgs/20190406-ddia-chapter09-consistency-and-consensus/9-2.jpg&#34; alt=&#34;9-2&#34; title=&#34;9-2&#34;&gt;&lt;/p&gt;
&lt;p&gt;在上图中，分为两种操作：针对某个值进行read和write操作。&lt;/p&gt;
&lt;p&gt;客户端A的第一次和最后一次read操作，分别返回0和1，这没有问题，因为在这两次操作中间有客户端C的write操作将数据x更新为了1。&lt;/p&gt;
&lt;p&gt;但是，在写操作还在进行的时候，如果读操作返回的值会来回的跳变，即某次读请求返回的是旧值，而某一次又返回的是新值，这对于一个可线性化系统而言是不可接受的。&lt;/p&gt;
&lt;p&gt;为此，需要加入一个约束条件，如下图所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.codedump.info/media/imgs/20190406-ddia-chapter09-consistency-and-consensus/9-3.jpg&#34; alt=&#34;9-3&#34; title=&#34;9-3&#34;&gt;&lt;/p&gt;
&lt;p&gt;在上图中，箭头表示时序依赖关系。即先有客户端A的第二次read(x)操作，再有客户端B的第二次read(x)操作。客户端A的第二次读请求返回了x的新值1，而客户端B在这次读请求之后也去读x的值，此时应该返回的也是新值1。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;即：在一个可线性化的系统中，有一个很重要的约束条件，在写操作开始和结束之间必然存在一个时间段，此时读到x的值会在旧值与新值之间跳变。但是，如果某个客户端的读请求返回了新值，那么即使这时写操作还未真正完成，后续的所有读请求也应该返回新值。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;以下的例子进一步解释可线性化的操作，除了读写之外又引入另一种操作：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;cas(x, old, new)：表示一次原子的比较-设置操作（compare-and-set，简称CAS），如果此时x的值为old，则原子设置这个值为new；否则保留原有值不变，这个操作的返回值表示这次x原有的值是否为old，即设置操作是否发生。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://www.codedump.info/media/imgs/20190406-ddia-chapter09-consistency-and-consensus/9-4.jpg&#34; alt=&#34;9-4&#34; title=&#34;9-4&#34;&gt;&lt;/p&gt;
&lt;p&gt;上图中的每个操作都有一个竖线，表示可能的执行时间点。可线性化要求，连接这些标记的竖线，必须总是按时间（即从左到右）向前移动，而不能向后移动。因此，一旦新值被写入或读取，所有后续的值读到的都是新值，直到被覆盖。&lt;/p&gt;
&lt;p&gt;在上图中，有一些细节需要注意：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;客户端B首先read(x)，接下来客户端D write(x,0)，然后客户端A在write(x,1)，而最终返回给客户端B的值是1（客户端A写入的值）。这个结果是可能的，这意味着数据库执行的顺序是：先处理客户端D的写请求，然后是A的写入操作，最后才是B的读请求。虽然这个顺序并不是上面请求的顺序，但是考虑到请求有网络延迟的情况，比如可能B的请求延迟很大，导致在两次写请求之后才打到数据库，因此只能返回最后A写入的值。&lt;/li&gt;
&lt;li&gt;客户端A在收到写请求的应答之前，B就收到了新的值1，这表明写入成功。这也是可能的，这并不意味着B的读请求在A的写请求之前发生，只是意味着由于网络延迟等原因导致A稍后才收到响应。&lt;/li&gt;
&lt;li&gt;客户端的最后一次读取不满足线性化。因为在此之前，A已经读到了由C进行cas(x,2,4)操作设置的新值4，B的最后一次读请求在A读取到4之后，因此B不能读到旧值2了。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;注意可线性化（Lineariazability）和可串行化（Serializability）的区别：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可串行化：可串行化是事务的隔离属性，其中每个事务可以读写多个对象。用来确保事务执行的结果与串行执行的结果完全相同，即使串行执行的顺序可能与事务实际执行顺序不同。&lt;/li&gt;
&lt;li&gt;可线性化：可线性化是读写寄存器（单个对象）的最新值保证，并不要求将操作组合到事务中，因此无法避免写倾斜等问题。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;数据库可以同时支持可串行化与可线性化，这种组合又被称为严格的可串行化或者强的单副本可串行化（strong one-copy Serializability）。&lt;/p&gt;
&lt;h2 id=&#34;线性化的依赖条件&#34;&gt;线性化的依赖条件&lt;/h2&gt;
&lt;h1 id=&#34;实现线性化系统&#34;&gt;实现线性化系统&lt;/h1&gt;
&lt;p&gt;由于线性化本质上意味着“表现的好像只有一个数据副本，其上的操作都是原子操作”。最简单的方案就是只用一个数据副本，但是这样无法容错。&lt;/p&gt;
&lt;p&gt;系统容错最常见的方法是采用复制机制，回顾一下之前的多种复制方案：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;主从复制（部分支持可线性化）：主从复制系统中，只有主节点写入数据，而从节点保存副本数据。如果从主节点或者同步更新的从节点读取数据，则可以满足线性化。&lt;/li&gt;
&lt;li&gt;共识算法（可线性化）。&lt;/li&gt;
&lt;li&gt;多主复制（不可线性化）：用于同时在多个节点上执行写入操作，并将数据异步复制到其他节点，因此可能产生写入冲突。&lt;/li&gt;
&lt;li&gt;无主复制（可能不可线性化）：对于无主节点复制的系统，依赖于具体的quorum配置，以及如何定义强一致性，可能并不能保证线性化。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;线性化与quorum&#34;&gt;线性化与quorum&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://www.codedump.info/media/imgs/20190406-ddia-chapter09-consistency-and-consensus/9-6.jpg&#34; alt=&#34;9-6&#34; title=&#34;9-6&#34;&gt;&lt;/p&gt;
&lt;p&gt;上图中，x的初始值为0，写客户端向所有三副本（n=3，w=3）写入更新x为1。而客户端A从两个节点（r=2）读数据，其中一个节点返回1，而客户端B则从两个节点都得到了0。&lt;/p&gt;
&lt;p&gt;显然这是违反线性化要求的：客户端B在客户端A之后读取数据，但是仍然得到了旧值。&lt;/p&gt;
&lt;p&gt;总而言之，最安全的假定是类似Dynamo风格的无主复制系统无法保证线性化。&lt;/p&gt;
&lt;h2 id=&#34;线性化的代价&#34;&gt;线性化的代价&lt;/h2&gt;
&lt;h3 id=&#34;cap理论&#34;&gt;CAP理论&lt;/h3&gt;
&lt;p&gt;在一个数据中心内部，主要存在不可靠的网络，就可能会违背线性化的风险，需要做出权衡考虑：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果应用要求线性化，但是由于网络的原因，某些副本和其他副本断开连接之后无法继续处理请求，就必须等待网络恢复，或者直接返回错误。无论哪种方式，结果都是服务不可用。&lt;/li&gt;
&lt;li&gt;如果应用不要求线性化，那么断开连接之后，每个副本可独立处理请求，此时服务可用但是行为不符合线性化。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因此，不要求线性化的应用更能容忍网络故障。这种思路称为“CAP定力”。&lt;/p&gt;
&lt;p&gt;CAP定理，表示一致性、可用性、分区容错性，三者之间只能同时满足两个特性。不过，这种表示具有误导性，因为网络分区是一种故障，不管喜欢与否，都可能发生，而无法选择或者逃避这个问题。&lt;/p&gt;
&lt;p&gt;网络在发生故障以后，必须从一致性和可用性之间做出选择。因此，更准确的应该是“网络分区情况下，选择一致还是可用”。&lt;/p&gt;
&lt;h1 id=&#34;顺序保证&#34;&gt;顺序保证&lt;/h1&gt;
&lt;p&gt;因果关系对所发生的事件施加了某种顺序：发送消息先于收到消息，问题出现在答案之前等。&lt;/p&gt;
&lt;p&gt;如果系统满足因果关系所规定的顺序，称之为“因果一致性（causally consistent）”。&lt;/p&gt;
&lt;h2 id=&#34;因果顺序并非全序&#34;&gt;因果顺序并非全序&lt;/h2&gt;
&lt;p&gt;全序关系（total order）支持任何两个元素之间进行比较，即对于任意元素，总是可以指出哪个大哪个小。&lt;/p&gt;
&lt;p&gt;但是有些集合并不符合全序关系，例如集合{a,b}大于集合{b,c}么？因为它们都不是对方的子集，所以无法直接进行比较。这种情况称之为不可比较（incomparable），数学集合只能是偏序关系（partially ordered）。&lt;/p&gt;
&lt;p&gt;全序和偏序的差异也体现在数据库一致性问题中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可线性化：在一个可线性化的系统中，存在全序操作关系。系统的行为好像就只有一个数据副本，且每个操作都是原子的，这意味着任何两个操作，都可以指出操作的先后顺序来。&lt;/li&gt;
&lt;li&gt;因果关系：如果两个操作都没有发送在对方之前，那么两个操作是并发关系（concurrent）。即，如果两个事件是因果关系，那么这两个事件就可以被排序；而并发的事件则无法排序比较。因此因果关系是偏序，而非全序。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;根据上面的定义，在可线性化的系统中不存在并发操作。一定有一个时间线将所有操作都全序执行，可能存在多个请求处于等待处理的状态，但是数据存储保证了在特定的时间点执行特定的操作，所以是单个时间轴、单个数据副本，没有并发。&lt;/p&gt;
&lt;p&gt;并发意味着时间线会出现分支和合并，而不同分支上的操作无法直接比较，一定有一个时间线将所有操作都全序执行。&lt;/p&gt;
&lt;h2 id=&#34;可线性化强于因果一致性&#34;&gt;可线性化强于因果一致性&lt;/h2&gt;
&lt;p&gt;可线性化意味着一定满足因果关系，任何可线性化的系统一定能够正确满足因果关系。&lt;/p&gt;
&lt;p&gt;在许多情况下，系统只要能够满足因果一致性就足够了，可线性化的代价太高。&lt;/p&gt;
&lt;h2 id=&#34;序列号排序&#34;&gt;序列号排序&lt;/h2&gt;
&lt;p&gt;可以使用序列号或时间戳来排序事件。时间戳不一定来自物理时钟，可以只是逻辑时钟。&lt;/p&gt;
&lt;p&gt;特别地，还可以按照与因果关系一致的顺序来创建序列号：保证如果操作A发生在B之前，那么A的序列号一定比B更小。&lt;/p&gt;
&lt;p&gt;在主从复制数据库中，复制日志中可以定义与因果关系一致的写全序关系，即由主节点为每个操作递增计数器，从而系统中的每个操作都赋值一个单调递增的序列号。&lt;/p&gt;
&lt;p&gt;但是如果系统中不存在唯一的主节点，比如是多主或无主类型的数据库，可以采用以下的方式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;每个节点独立生成自己的一组序列号，比如两个节点一个节点生成奇数，另一个节点生成偶数。另外还可以在序列号中加入所属节点的唯一标识，确保不同的节点用于不会生成相同的序列号。&lt;/li&gt;
&lt;li&gt;可以把时间戳信息（物理时钟）加到每个操作上。&lt;/li&gt;
&lt;li&gt;可以预先分配序列号的区间范围。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;lamport时间戳&#34;&gt;Lamport时间戳&lt;/h2&gt;
&lt;p&gt;如下图所示，每个节点都有唯一的标识符，且每个节点都有一个计数器来记录自己处理的请求总数，Lamport时间戳是一个值对：（计数器，节点ID），这样就能确保每个Lamport时间戳都是唯一的。&lt;/p&gt;
&lt;p&gt;给定两个Lamport时间戳，可以这样来对比得到全序关系：计数器大的时间戳大，如果计数器相同，那么节点ID大的时间戳更大。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.codedump.info/media/imgs/20190406-ddia-chapter09-consistency-and-consensus/9-8.jpg&#34; alt=&#34;9-8&#34; title=&#34;9-8&#34;&gt;&lt;/p&gt;
&lt;p&gt;Lamport时间戳与版本向量的区别在于：版本向量用于区分两个操作是并发的还是因果依赖的，而Lamport时间戳用于确保全序关系。即使Lamport时间戳不能用于区分两个操作属于并发关系，还是因果依赖关系。&lt;/p&gt;
&lt;p&gt;但是，即便有了全序的时间戳排序，有一些问题仍然无法解决。&lt;/p&gt;
&lt;p&gt;比如注册一个网站时，要求用户名需要唯一，虽然两个同样名字的创建用户请求过来，可以根据全序关系来决定究竟哪个请求在先抢占了这个用户名，但是这并不够，因为这个是在请求写入之后才进行的判断，在应答写请求时无法立刻知道结果，因为还需要查询所有节点，如果有节点失败的情况下还需要等待，等等。&lt;/p&gt;
&lt;p&gt;为了解决类似的问题，就需要引入”全序关系广播“这个概念了。&lt;/p&gt;
&lt;h2 id=&#34;全序关系广播total-oder-broadcast&#34;&gt;全序关系广播（Total Oder Broadcast）&lt;/h2&gt;
&lt;p&gt;全序关系广播指节点间交换消息的某种协议，要求满足以下两个基本安全属性：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可靠发送：没有消息丢失，如果消息发送到了一个节点，也必须要发送到其他节点。&lt;/li&gt;
&lt;li&gt;严格有序：消息总是以相同的顺序发送给每个节点。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;即使节点或者网络发生故障，全序关系广播算法的正确实现也必须保证上述两条。在网络中断时发送失败的消息，在恢复之后要继续重试。&lt;/p&gt;
&lt;p&gt;全序关系广播正是数据库复制所需要的：如果每条消息代表数据库写请求，并且每个副本都按照相同的顺序处理这些写请求，那么所有副本可以保持一致。该原则称为“状态机复制”，ZK和etcd这样的服务就实现了全序关系广播。&lt;/p&gt;
&lt;p&gt;理解全序关系广播的另一种方式是将其视为日志（如复制日志），传递消息就像追加方式更新日志。由于所有节点必须以相同的顺序发送消息，因此所有节点都可以获取日志并看到相同的消息序列。&lt;/p&gt;
&lt;h1 id=&#34;分布式事务与共识&#34;&gt;分布式事务与共识&lt;/h1&gt;
&lt;h3 id=&#34;两阶段提交two-phase-commit简称2pc&#34;&gt;两阶段提交（two-phase commit，简称2PC）&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://www.codedump.info/media/imgs/20190406-ddia-chapter09-consistency-and-consensus/9-9.jpg&#34; alt=&#34;9-9&#34; title=&#34;9-9&#34;&gt;&lt;/p&gt;
&lt;p&gt;以上是简单的2PC的操作示意图，图中引入了一个协调者（Coordinator）的角色。当应用程序开始提交事务时，协调者开始阶段1：发送一个准备请求给事务中的参与者，询问是否可以提交。协调者然后跟踪参与者的回应：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果所有参与者都应答”是“，表示它们已经准备好提交，协调者接下来在阶段2发出提交请求，提交才开始执行。&lt;/li&gt;
&lt;li&gt;如果任何参与者回答了”否“，则协调者在阶段2中向所有节点发送放弃请求。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果参与者在2PC期间失败，那么协调者将中断事务提交；如果在第二阶段发送提交时失败，协调者将无限期重试。&lt;/p&gt;
&lt;h3 id=&#34;2pc的原理&#34;&gt;2PC的原理&lt;/h3&gt;
&lt;p&gt;以下详细分解2PC流程：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;启动分布式事务时，首先向协调者请求事务ID，该ID全局唯一。&lt;/li&gt;
&lt;li&gt;在每个参与节点上执行单节点事务，并将全局唯一事务ID附加到事务上。此时，读写都是在单节点内完成的，如果这个阶段出现问题，如节点崩溃或者请求超时，则协调者和其他参与者都可以安全中止。&lt;/li&gt;
&lt;li&gt;当准备提交事务时，协调者向所有参与者发送准备请求，并附带全局事务ID，如果准备请求有任何一个发生问题，协调者可以通知参与者放弃事务。&lt;/li&gt;
&lt;li&gt;参与者在收到准备请求之后，确保在任何情况下都可以提交事务，包括安全地将事务数据写入磁盘。一旦协调者回答“是”，节点就提交事务。&lt;/li&gt;
&lt;li&gt;当协调者收到所有准备请求的答复时，就是否提交（或放弃） 事务做出明确的决定（即只有所有参与者都投赞成票时才会提交）。协调者将最后的决定写入磁盘，这个时刻称为“提交点（commit point）”。&lt;/li&gt;
&lt;li&gt;协调者决定写入磁盘之后，向所有参与者发送提交请求。如果请求失败，就需要一直重试。此时，所有节点不允许有任何反悔。一旦做出了决定，就必须执行，即使需要重试。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;由此可见，该协议两个关键的“不归路”：首先，当参与者投票“是”时，做了肯定提交的承诺；其次，协调者做了提交的决定之后，这个决定也是不可撤销的。&lt;/p&gt;
&lt;p&gt;正是以上两点保证了2PC的原子性，而单节点事务提交实际上将两个事件合二为一，写入事务日志即提交。&lt;/p&gt;
&lt;h3 id=&#34;协调者发生故障&#34;&gt;协调者发生故障&lt;/h3&gt;
&lt;p&gt;但是，如果是协调者自身发生了故障，后面的行为无法预计，如下图所示。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.codedump.info/media/imgs/20190406-ddia-chapter09-consistency-and-consensus/9-10.jpg&#34; alt=&#34;9-10&#34; title=&#34;9-10&#34;&gt;&lt;/p&gt;
&lt;p&gt;2PC能够顺利完成的唯一方法是等待协调者恢复，这就是为什么协调者必须在向参与者发送提交请求之前需要先写入磁盘事务的日志：这是因为一旦协调者崩溃，恢复之后可以根据读取事务日志来确定所有未决的事务。&lt;/p&gt;
&lt;h1 id=&#34;支持容错的共识算法&#34;&gt;支持容错的共识算法&lt;/h1&gt;
&lt;p&gt;所有支持容错的共识算法都有以下的性质：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;协商一致性（Uniform agreement）：所有的节点都接受相同的决议。&lt;/li&gt;
&lt;li&gt;诚实性（Integrity）：所有节点都不能反悔，即对一项决议不能有两次不同的结果。&lt;/li&gt;
&lt;li&gt;合法性（Validity）：如果决定了v值，则v一定是某个节点所提议的。即：不能有一个凭空的决议产生。&lt;/li&gt;
&lt;li&gt;可终止性（Termination）：节点如果不崩溃则最终一定可以达成协议。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;协商一致性和诚实性属性定义了共识算法的核心思想：决定一致的结果，而一旦决定就不能再变更决定。
有效性属性排除了无意义的方案。&lt;/p&gt;
&lt;p&gt;如果不考虑容错性，以上三点很容易实现：强行指定某个节点为”独裁者“，由它做出所有的决定。但是，如果该节点失败，系统就无法再继续做出任何决定。这就是在2PC时看到的：如果协调者失败了，那些处于不确定状态的参与者无从知道应该怎么做。&lt;/p&gt;
&lt;p&gt;可终止性引入了容错的思想。它强调一个共识算法不能原地空转，永远不做事情。即使某些节点出现了故障，其它节点也必须最终做出决定。&lt;/p&gt;
&lt;p&gt;因此，可终止性属于一种活性属性（liveness property），而其它三个性质属于安全性方面的属性。&lt;/p&gt;
&lt;p&gt;任何共识性算法，都需要至少大部分节点正确运行才能保证终止性，这个”大多数节点“又被称为”quorum“。&lt;/p&gt;
&lt;p&gt;因此，可终止性的前提是，发生崩溃或者不可用的节点必须小于小半数节点。另外，共识算法也界定系统不存在拜占庭错误。&lt;/p&gt;
&lt;h2 id=&#34;共识算法与全序广播&#34;&gt;共识算法与全序广播&lt;/h2&gt;
&lt;p&gt;共识算法一般都是：决定了一系列值，然后采用全序关系广播算法传播数据。&lt;/p&gt;
&lt;p&gt;全序广播的要点是：消息按照相同的顺序发送到所有节点，有且只有一次。&lt;/p&gt;
&lt;p&gt;所以，全序广播算法相当于持续的多轮共识：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;由于协商一致性，所有节点决定以相同顺序发送相同的消息。&lt;/li&gt;
&lt;li&gt;由于诚实性：消息不能重复。&lt;/li&gt;
&lt;li&gt;由于合法性，消息不能被破坏和捏造。&lt;/li&gt;
&lt;li&gt;由于可终止性，消息不能丢失。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;主从复制与共识&#34;&gt;主从复制与共识&lt;/h2&gt;
&lt;p&gt;主从复制，也是所有写入操作由主节点负责，并以相同顺序发送到从节点来保持副本数据更新，为什么那时候没有考虑共识问题？&lt;/p&gt;
&lt;p&gt;如果主节点由人手动选择和配置，那就是一个独裁性质的一致性算法，出现故障的时候需要人工干预。&lt;/p&gt;
&lt;p&gt;然而，共识算法由需要首先选择出一个主节点来，否则会出现脑裂问题。如何选举主节点呢？&lt;/p&gt;
&lt;h2 id=&#34;epoch和quorum&#34;&gt;epoch和quorum&lt;/h2&gt;
&lt;p&gt;共识算法中，每个协议会定义一个世代编号（epoch number），这个编号是递增唯一的，对应于paxos中的ballot number、vsp中的view number、raft中的term number。&lt;/p&gt;
&lt;p&gt;当主节点失效时，马上进行一轮新的投票来选举出新的主节点。选举会赋予一个单调递增的epoch号，如果出现不同的主节点，那么就看谁的epoch号更大的胜出。&lt;/p&gt;
&lt;p&gt;在主节点做出任何决定之前，必须首先检查是否存在比它更高的epoch号，如何检查呢？基于前面做分布式系统的一个准则”真理由多数决定“，节点不能依靠自己掌握的信息来决策，而应该从quorum节点中收集投票。节点只有当没有发现更高epoch的主节点存在的情况下，才会对当前的提议进行投票。&lt;/p&gt;
&lt;p&gt;因此实际上这里是两轮不同的投票：首先投票决定谁是主节点，然后是对主节点的提议进行投票。&lt;/p&gt;
&lt;p&gt;投票过程看起来像2PC，区别在于：2PC的协调者不是依靠选举产生；另外共识算法只需要收到quorum节点的应答就可以通过决议，而2PC需要所有参与者都通过才能通过决议。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>《数据密集型应用系统设计》第八章《分布式系统的挑战》笔记</title>
      <link>https://www.codedump.info/post/20190405-ddia-chapter08-the-trouble-with-distributed-system/</link>
      <pubDate>Tue, 16 Apr 2019 21:03:16 +0800</pubDate>
      
      <guid>https://www.codedump.info/post/20190405-ddia-chapter08-the-trouble-with-distributed-system/</guid>
      
      <description>&lt;p&gt;本章描述分布式系统中可能出现的各种问题。&lt;/p&gt;
&lt;h1 id=&#34;故障与部分失效&#34;&gt;故障与部分失效&lt;/h1&gt;
&lt;p&gt;单机上的程序，以一种确定性的方式运行：要么工作，要么出错。&lt;/p&gt;
&lt;p&gt;然而涉及到多台节点时，会出现系统的一部分正常，一部分异常的情况，称为“部分故障（partial failure）”。&lt;/p&gt;
&lt;p&gt;正是由于这种不确定性和部分失效大大提高了分布式系统的复杂性。&lt;/p&gt;
&lt;h1 id=&#34;不可靠的网络&#34;&gt;不可靠的网络&lt;/h1&gt;
&lt;p&gt;分布式系统中的多个节点以网络进行通信，但是网络并不保证什么时候到达以及是否一定到达。等待响应的过程中，很多事情可能出错：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;请求可能丢失。&lt;/li&gt;
&lt;li&gt;请求在某个队列里等待，无法马上发送。&lt;/li&gt;
&lt;li&gt;远程节点因为崩溃、宕机等原因已经失效。&lt;/li&gt;
&lt;li&gt;远程节点因为某些原因暂时无法响应。&lt;/li&gt;
&lt;li&gt;远程节点接收并且处理了请求，但是回复却丢失了。&lt;/li&gt;
&lt;li&gt;远程节点已经完成了请求，但是回复被延迟了。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://www.codedump.info/media/imgs/20190405-ddia-chapter08-the-trouble-with-distributed-system/8-1.jpg&#34; alt=&#34;8-1&#34; title=&#34;8-1&#34;&gt;&lt;/p&gt;
&lt;p&gt;在上图中，请求没有得到响应，但是无法区分是因为什么原因，可能有：请求丢失、远程节点关闭、响应丢失等情况。&lt;/p&gt;
&lt;p&gt;从以上可以知道，异步网络中的消息没有得到响应，但是无法判断具体的原因。&lt;/p&gt;
&lt;p&gt;处理这种问题通常采用超时机制：在等待一段时间之后，如果没有收到回复则选择放弃，并且认为响应不会到达。&lt;/p&gt;
&lt;h2 id=&#34;检测网络故障&#34;&gt;检测网络故障&lt;/h2&gt;
&lt;p&gt;如果超时是检测网络故障的唯一可行方法，那么这个超时时间应该如何选择？&lt;/p&gt;
&lt;p&gt;太小：出现误判的情况。太大：意味着要很长时间才能宣布节点失效了。&lt;/p&gt;
&lt;p&gt;假设有一个虚拟的系统，网络可以保证数据报在一个最大延迟范围内：要么在时间d内交付完成，要么丢失。此外，非故障节点在时间r内完成请求的处理。此时，就可以确定成功的请求总是在2d+r时间内完成，因此这个时间是一个理想超时时间。&lt;/p&gt;
&lt;h2 id=&#34;同步网络和异步网络&#34;&gt;同步网络和异步网络&lt;/h2&gt;
&lt;p&gt;既然同步网络可以在规定的延迟时间内完成数据的发送，且不会丢失数据包，那么为什么分布式系统没有选择同步网络，在硬件层面就解决网络问题？&lt;/p&gt;
&lt;p&gt;原因在于，固定电话网络中的电路与TCP连接存在很大的不同：电路方式总是预留固定带宽，在电路建立之后其他人无法使用；而TCP连接的数据包则会尝试使用所有可用的网络带宽。TCP可以传送任意大小可变的数据块，会尽力在最短时间内完成数据传送。&lt;/p&gt;
&lt;h1 id=&#34;不可靠的时钟&#34;&gt;不可靠的时钟&lt;/h1&gt;
&lt;p&gt;很多操作依赖时间，但是时间也是靠不住的，本节就是说这部分的内容。&lt;/p&gt;
&lt;p&gt;计算机的时钟分为两种，墙上时钟（time-of-day clock）和单调时钟（monotonic clock），但是两者在使用上是有区别的。&lt;/p&gt;
&lt;p&gt;墙上时钟根据某个日历（也称为墙上时间，wall-clock time）返回当前的日期和时间。比如Linux的系统调用clock_gettime(CLOCK_REALTIME)返回自1970年1月1日以来的秒数和毫秒数。&lt;/p&gt;
&lt;p&gt;单调时钟更适合用于测试持续时间段（时间间隔），Linux的系统调用clock_gettime(CLOCK_MONITONIC)返回的就是单调时钟。单调时钟的名字源于它们总是保证向前走而不会出现回拨现象。&lt;/p&gt;
&lt;p&gt;可以在一个时间点读取单调时钟的值，完成某项工作然后再次检查时钟，时钟之间的插值就是两次检查的时间间隔。&lt;/p&gt;
&lt;p&gt;但是，单调时钟的绝对值没有任何意义。&lt;/p&gt;
&lt;p&gt;单调时钟不需要同步，而墙上时钟需要根据NTP服务器或外部时间源做调整。&lt;/p&gt;
&lt;h2 id=&#34;依赖时钟的同步&#34;&gt;依赖时钟的同步&lt;/h2&gt;
&lt;p&gt;某些操作强依赖时钟的同步，这里往往容易出现问题，这一节就是列举这些问题。&lt;/p&gt;
&lt;h3 id=&#34;时间戳与事件顺序&#34;&gt;时间戳与事件顺序&lt;/h3&gt;
&lt;p&gt;一个常见的功能：跨节点的事件排序，如果高度依赖时钟计时，就存在一定的技术风险。比如，两个客户端同时写入数据库，谁先到达，哪个操作是最新的？&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.codedump.info/media/imgs/20190405-ddia-chapter08-the-trouble-with-distributed-system/8-3.jpg&#34; alt=&#34;8-3&#34; title=&#34;8-3&#34;&gt;&lt;/p&gt;
&lt;p&gt;上图中，客户端A写入x=1的时间是42.004秒，而客户端B写入x+=1即x=2虽然在后面发生但是时间是42.003秒。节点2在收到这两个事件时，会根据时间戳错误的认为x=1是最新的值，丢弃了x=2的值。&lt;/p&gt;
&lt;p&gt;这种冲突解决方式称为“最后写入获胜（Last Write Win）”，但是这样保持“最新”值并丢弃其他值的做法，由于“最新”的定义强依赖于墙上时钟，则会引入偏差。&lt;/p&gt;
&lt;h3 id=&#34;时钟的置信区间&#34;&gt;时钟的置信区间&lt;/h3&gt;
&lt;p&gt;不应该把墙上时间视为一个精确的时间点，而更应该被视为带有置信区间的时间范围。比如，系统有95%的置信度认为目前时间在[10.3,10.5]秒之间。&lt;/p&gt;
&lt;p&gt;比如Google Spanner中的TrueTime API，在查询当前时间时，会得到两个值：[不早于，不晚于]分别代表误差的最大偏差范围。&lt;/p&gt;
&lt;h2 id=&#34;进程暂停&#34;&gt;进程暂停&lt;/h2&gt;
&lt;p&gt;另外一个分布式系统中危险使用时钟的例子：假设数据库每个分区只有一个主节点，只有主节点可以接收写入，那么其它节点该如何确信该节点没有被宣告失效，可以继续安全写入呢？&lt;/p&gt;
&lt;p&gt;一种思路是主节点从其它节点获得一个租约，类似一个带有超时的锁。某一个时间只有一个节点可以拿到租约，某节点获得租约之后，在租约到期之前，它就是这段时间内的主节点。为了维持主节点的身份，节点必须在到期之前定期去更新租约。如果节点发生了故障，则续约失败，这样另一个节点到期之后就可以接管。&lt;/p&gt;
&lt;p&gt;典型流程类似这样：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.codedump.info/media/imgs/20190405-ddia-chapter08-the-trouble-with-distributed-system/8-renew-lease.jpg&#34; alt=&#34;8-renew-lease&#34; title=&#34;8-renew-lease&#34;&gt;&lt;/p&gt;
&lt;p&gt;这段代码有几个问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;依赖于同步的时钟，租约到期时间由另一台机器锁设置，并和本地时间进行比较。如果两者有比较大的误差则可能出现问题。&lt;/li&gt;
&lt;li&gt;代码中假定了检查点的system.currentTimeMillis()和请求处理process(request)间隔很短。但是，如果进程由于GC等原因被暂停，也有可能发生问题。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;知识真相与谎言&#34;&gt;知识、真相与谎言&lt;/h1&gt;
&lt;p&gt;以上阐述了分布式系统中的网络、时钟都不是很靠谱，那么分布式系统中什么信息才具有较大的可信度呢？&lt;/p&gt;
&lt;p&gt;在分布式系统中，我们可以明确列出对系统行为（系统模型）的若干假设，然后以满足这些假设条件来为目标构建实际运行的系统。在给定系统模型下，可以验证算法的正确性。这也意味着即使底层模型仅提供少数几个保证，也可以在系统软件层面实现可靠的行为保证。&lt;/p&gt;
&lt;h2 id=&#34;真相由多数决定&#34;&gt;真相由多数决定&lt;/h2&gt;
&lt;p&gt;节点不能根据自己的信息来判断自身的状态。由于节点可能随时会失效，可能会暂停、假死，甚至最终无法恢复，因此分布式系统不能完全依赖于单个节点。目前，许多分布式算法都依靠法定票数，即在节点之间进行投票。任何决策都需要来自多个节点的最小投票数，从而减少对特定节点的依赖。&lt;/p&gt;
&lt;p&gt;这其中也包括宣告某个节点失效。如果有法定数量的节点声明另一个节点失效，即使该节点仍然感觉自己存活，也必须接受失效的裁定进行下线操作。&lt;/p&gt;
&lt;h2 id=&#34;主节点与锁&#34;&gt;主节点与锁&lt;/h2&gt;
&lt;p&gt;有很多情况下，需要在系统范围内确保只有一个实例，比如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;只允许一个节点做为数据库分区的主节点，以防止出现脑裂现象。&lt;/li&gt;
&lt;li&gt;只允许一个事务或客户端持有特定资源的锁，以防止同时写入。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在分布式系统中，即使某个节点自认为自己是“唯一的那个”，但并不一定系统中的多数节点都这么认为。当系统中的多数节点认为某节点已经失效，但是该节点还继续充当“唯一的那个”节点工作时，就可能出现问题。&lt;/p&gt;
&lt;p&gt;如下图中所示，客户端1的锁租约已经到期，但是仍然自认为有效，导致了数据被破坏。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.codedump.info/media/imgs/20190405-ddia-chapter08-the-trouble-with-distributed-system/8-4.jpg&#34; alt=&#34;8-4&#34; title=&#34;8-4&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;fencing与锁&#34;&gt;Fencing与锁&lt;/h2&gt;
&lt;p&gt;当使用锁和租约机制来保护资源的并发访问时，必须确保过期的“唯一的那个”节点不影响其他正常部分。要实现这一点，可以使用fencing（栅栏，隔离之意）技术。&lt;/p&gt;
&lt;p&gt;假设每次锁服务在授予锁或租约时，还会返回一个fencing令牌，该令牌每次授予都会递增。然后，客户端每次向存储系统发起写请求时，都必须包含所持有的fencing令牌。&lt;/p&gt;
&lt;p&gt;如下图所示，客户端1获得锁租约的时候得到了令牌33，随后陷入长时间暂停直到租约到期。此时客户端2获得了新的锁租约和令牌34。客户端1恢复之后尝试进行写请求，但是此时带上的令牌33小于34，所以被拒绝写操作。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.codedump.info/media/imgs/20190405-ddia-chapter08-the-trouble-with-distributed-system/8-5.jpg&#34; alt=&#34;8-5&#34; title=&#34;8-5&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;拜占庭故障&#34;&gt;拜占庭故障&lt;/h2&gt;
&lt;p&gt;fencing令牌可以用于检测并阻止无意的误操作，但是当节点有意故意破坏系统时，在发送消息时就可以故意伪造令牌了。&lt;/p&gt;
&lt;p&gt;在不信任的环境中需要达成共识的问题被称为拜占庭将军问题。&lt;/p&gt;
&lt;h1 id=&#34;理论系统模型与现实&#34;&gt;理论系统模型与现实&lt;/h1&gt;
&lt;p&gt;算法的实现不能过分依赖特定的硬件和软件配置。这就要求我们对预期的系统错误进行形式化描述，通过定义一些系统模型来形式化描述算法的前提条件。&lt;/p&gt;
&lt;p&gt;在计时方面，有三种常见的模型：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;同步模型：同步模型假定有上界的网络延迟，有上界的进程暂停和有上界的时钟误差。注意，这并不意味着完全同步的时钟或网络延迟为0.只是意味着清楚的了解网络延迟、暂停和时钟漂移不会超过某个固定的上界。&lt;/li&gt;
&lt;li&gt;部分同步模型：部分同步意味着系统在大多数情况下像一个同步系统一样运行，但有时候会超出网络延时，进程暂停和时钟漂移的预期上界。这是一个比较现实的模型：大多数情况下，网络和进程比较稳定，但是必须考虑到任何关于时机的假设都有偶尔违背的情况，而一旦发生，网络延迟、暂停和时钟偏差都可能会变得非常大。&lt;/li&gt;
&lt;li&gt;异步模型：在这个模型中，一个算法不会对时机做任何假设，甚至里面根本没有时钟（也没有超时机制）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;除了计时模型，还需要考虑到节点失效，有以下三种常见的节点失效系统模型：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;崩溃中止模型：在这个模型中，算法假设一个节点以一种方式发生故障，即遭遇系统崩溃。这意味着节点可能在任何时候突然停止响应，且该节点以后永远消失，无法恢复。&lt;/li&gt;
&lt;li&gt;崩溃恢复模型：节点可能在任何时候发生崩溃，且可能在一段（未知的）时间之后得到恢复并再次响应。在崩溃恢复模型中，节点上持久化存储的数据会在崩溃之后保存，而内存中的状态会丢失。&lt;/li&gt;
&lt;li&gt;拜占庭失效模型：节点可能发生任何问题，包括试图作弊和欺骗其它节点。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;算法的正确性&#34;&gt;算法的正确性&lt;/h2&gt;
&lt;p&gt;为了定义算法的正确性，需要描述它的属性信息，例如对于fencing令牌生成算法，有如下属性：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;唯一性：两个令牌请求不能获得相同的值。&lt;/li&gt;
&lt;li&gt;单调递增：如果请求x返回了令牌t1，请求y返回了令牌t2，且x在y开始之前先完成，那么t1&amp;lt;t2。&lt;/li&gt;
&lt;li&gt;可用性：请求令牌的节点如果不发生崩溃那么一定能收到响应。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;安全性safety和活性liveness&#34;&gt;安全性（safety）和活性（liveness）&lt;/h3&gt;
&lt;p&gt;有必要区分两种不同的属性：安全性和活性。在上面的例子中，唯一性和单调递增属于安全性，可用性属于活性。&lt;/p&gt;
&lt;p&gt;两种性质有何区别？活性的定义中通常包含暗示“最终”一词（最终一致性就是一种活性）。&lt;/p&gt;
&lt;p&gt;安全性可以理解为“没有发生意外”，活性类似“预期的事情最终一定会发生”。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果违反了安全性，可以明确指向发生的特定的时间点（例如，唯一性如果被违反，可以定位到具体哪个操作产生了重复令牌）。且一旦违反了安全性，违规行为无法撤销，破坏已实际发生。&lt;/li&gt;
&lt;li&gt;活性则反过来，可能无法明确某个具体的时间点（例如一个节点发送了一个请求，但还没有收到回应），但总是希望在未来某个时间点可以满足要求（即收到回复）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;区分安全性和活性的一个好处是可以帮助简化处理一些具有挑战性的系统模型。通常对于分布式算法，要求在所有可能的系统模型中，都必须满足安全性。也就是说，即使所有节点发生崩溃，或者整个网络中断，算法确保不能返回错误的结果。&lt;/p&gt;
&lt;p&gt;而对于活性，则存在一些必要条件。例如，只有在系统多数节点没有崩溃，以及网络最终可以恢复的前提下，才能保证可以收到响应。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>《数据密集型应用系统设计》第七章《事务》笔记</title>
      <link>https://www.codedump.info/post/20190403-ddia-chapter07-transaction/</link>
      <pubDate>Wed, 03 Apr 2019 22:33:58 +0800</pubDate>
      
      <guid>https://www.codedump.info/post/20190403-ddia-chapter07-transaction/</guid>
      
      <description>&lt;p&gt;事务提供了一种机制，应用程序可以把一组读和写操作放在一个逻辑单元里，所有在一个事务的读和写操作会被视为一个操作：要么全部失败，要么全部成功，因此应用程序不需要担心部分失败（partial failure）问题，可以安全的重试。&lt;/p&gt;
&lt;h1 id=&#34;深入理解事务&#34;&gt;深入理解事务&lt;/h1&gt;
&lt;p&gt;事务提供的安全性保证即所谓的&lt;code&gt;ACID&lt;/code&gt;，它包括以下四个要求：&lt;/p&gt;
&lt;h2 id=&#34;acid&#34;&gt;ACID&lt;/h2&gt;
&lt;h3 id=&#34;原子性atomicity&#34;&gt;原子性（Atomicity）&lt;/h3&gt;
&lt;p&gt;A（Atomicity，原子性）：在一个事务中的所有操作，要么全部成功，要么全部失败，不存在部分成功或者部分失败的情况。在出错时中断事务，前面成功的操作都会被丢弃。&lt;/p&gt;
&lt;h3 id=&#34;一致性consistency&#34;&gt;一致性（consistency）&lt;/h3&gt;
&lt;p&gt;C（Consistency，一致性）：对数据有特定的预期状态，任何数据修改必须满足这些状态约束，比如针对一个账号，账号上的款项必须保持平衡。&lt;/p&gt;
&lt;h3 id=&#34;隔离性isolation&#34;&gt;隔离性（isolation）&lt;/h3&gt;
&lt;p&gt;I（Isolation，隔离性）：并发执行的多个事务，不会相互影响。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.codedump.info/media/imgs/20190403-ddia-chapter07-transaction/7-1.jpg&#34; alt=&#34;7-1&#34; title=&#34;7-1&#34;&gt;
如上图中所示，两个客户端同时增加数据库的计时器，由于没有做好隔离，导致最终的结果是43而不是正确的44。&lt;/p&gt;
&lt;p&gt;ACID语义中的隔离性意味着并发执行的多个事务相互隔离，不能交叉运行。经典的数据库教材将隔离性定义为可串行化（serializability），这就意味着可以假装它是数据库上运行的唯一事务。&lt;/p&gt;
&lt;p&gt;然而实践中，由于性能问题很少使用串行化隔离。&lt;/p&gt;
&lt;h3 id=&#34;持久性durability&#34;&gt;持久性（Durability）&lt;/h3&gt;
&lt;p&gt;D（Durability，持久性）：一旦事务提交，数据将被持久化存储起来。&lt;/p&gt;
&lt;h1 id=&#34;弱隔离级别&#34;&gt;弱隔离级别&lt;/h1&gt;
&lt;p&gt;可串行化的隔离会影响性能，而很多业务不愿意牺牲性能，因而倾向于使用更弱的隔离级别。&lt;/p&gt;
&lt;p&gt;以下介绍几个常见的弱隔离级别（非串行化）。&lt;/p&gt;
&lt;h2 id=&#34;读提交read-committed&#34;&gt;读提交（read committed）&lt;/h2&gt;
&lt;p&gt;读提交是最基本的事务级别，提供两个保证：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;读数据库时，只能读到被提交成功的数据（不会读到脏数据）。&lt;/li&gt;
&lt;li&gt;写数据库时，只会覆盖已被提交成功的数据（不会脏写）。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;防止脏读&#34;&gt;防止脏读&lt;/h3&gt;
&lt;p&gt;如果一个事务被中断或者没有提交成功，而另一个事务能读取到这部分没有提交成功的数据，这就是“脏读”。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.codedump.info/media/imgs/20190403-ddia-chapter07-transaction/7-4.jpg&#34; alt=&#34;7-4&#34; title=&#34;7-4&#34;&gt;&lt;/p&gt;
&lt;p&gt;如上图，用户2仅在用户1的事务提交成功之后，才能读取到这次事务修改的新值x=3。&lt;/p&gt;
&lt;h3 id=&#34;防止脏写&#34;&gt;防止脏写&lt;/h3&gt;
&lt;p&gt;如果先前写入的数据是尚未提交事务的一部分，而被另一个事务的写操作覆盖了，这就是脏写。通常防止脏写的办法是推迟第二个写请求，等到前面的事务操作提交。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.codedump.info/media/imgs/20190403-ddia-chapter07-transaction/7-5.jpg&#34; alt=&#34;7-5&#34; title=&#34;7-5&#34;&gt;&lt;/p&gt;
&lt;p&gt;如上图，alice和bob两人试图购买同一辆车。购买时需要两次数据库写入：网站需要更新买主为新买家，而同时发票也需要随之更新。
但是在上图中，车主被改成了bob，但是发票上面写的却是alice。&lt;/p&gt;
&lt;h3 id=&#34;实现读提交&#34;&gt;实现读提交&lt;/h3&gt;
&lt;p&gt;实现防脏写：数据库通常使用行级锁来防止脏写，事务想修改某个对象，必须首先获得该对象的锁，直到事务结束。&lt;/p&gt;
&lt;p&gt;实现防脏读：也可以使用前面的防脏写来实现防脏读，但是这样代价太大了。一般的方式是保存这个值的两个版本，事务没有提交之前返回旧的值，提交之后才返回新的值。&lt;/p&gt;
&lt;p&gt;然而，读锁在实际中并不可行，原因在于运行时间较长的事务导致了许多只读事务等待太长的时间。&lt;/p&gt;
&lt;p&gt;因此，大部分数据库使用7-4中的方式来防止脏读：对于每个待更新的对象，数据库都会维护其旧值和当前持有锁事务将要设置的新值两个版本。在事务提交之前返回的是旧值；仅当事务提交之后，才会切换到新的值。&lt;/p&gt;
&lt;h3 id=&#34;快照隔离级别snapshot-isolation和重复读&#34;&gt;快照隔离级别（Snapshot isolation）和重复读&lt;/h3&gt;
&lt;p&gt;尽管上面的读提交已经能解决一部分问题，但是还是有一些问题不能解决的，如下图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.codedump.info/media/imgs/20190403-ddia-chapter07-transaction/7-6.jpg&#34; alt=&#34;7-6&#34; title=&#34;7-6&#34;&gt;&lt;/p&gt;
&lt;p&gt;上图中，alice有两个账号，但是如果alice在转账过程中去查看账户，会发现少了100美元。&lt;/p&gt;
&lt;p&gt;原因在于：alice对两个账户的两次读操作是同一个事务，而在这两次读操作之间，还有两次写操作，在这两次写操作完成之后才进行的第二次读操作，这样读出来的数据就不一致了。&lt;/p&gt;
&lt;p&gt;这种异常现象称为”不可重复读取（nonrepeatable read）“或者”读倾斜（read skew）“问题。&lt;/p&gt;
&lt;p&gt;以上问题，并不是一个永久性问题，因为alice的账号最终会一致，然而某些场景下这种过程中的不一致现象不能接受，比如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;备份数据：如果备份过程中的数据不一致，就会导致永久的不一致。&lt;/li&gt;
&lt;li&gt;分析查询与完整性检查场景：如果这些查询在不同的时间点返回不一致的结果，则结果也无意义。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;快照隔离级别是解决以上问题的常见手段。每个事物都从数据库的一致性快照中读取，事务一开始看到的是最近所提交的数据，即使数据随后可能被另一个事务修改，但保证事务都只能看到该特定时间点的旧数据。&lt;/p&gt;
&lt;p&gt;快照隔离级别对于长时间运行的只读查询（如备份和分析）非常有用。如果数据在查询的同时还在发生变化，那么查询结果对应的物理含义就难以理清。而如果查询的是数据库在某时刻点所冻结的一致性快照，则查询结果非常明确。&lt;/p&gt;
&lt;h3 id=&#34;实现快照隔离级别&#34;&gt;实现快照隔离级别&lt;/h3&gt;
&lt;p&gt;快照级隔离的实现通常采用写锁来防止脏写，这意味着正在进行写操作的事务会阻止同一对象上其他事务。而读锁则不需要加锁了。从性能角度，快照级别隔离的关键点就是读操作不会阻止写操作，反之亦然。这使得数据库可以在处理正常写入的同时，在一致性快照上执行长时间的只读查询，且两者之间没有锁的竞争。&lt;/p&gt;
&lt;p&gt;考虑到多个正在进行的事务可能会在不同的时间点查看数据库状态，所以数据库保留了对象的多个不同的提交版本，称为MVCC（Multi Version Concurrency Control）。如下图所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.codedump.info/media/imgs/20190403-ddia-chapter07-transaction/7-7.jpg&#34; alt=&#34;7-7&#34; title=&#34;7-7&#34;&gt;&lt;/p&gt;
&lt;p&gt;给每个事务一个唯一的、单调递增的事务ID（txid），每当事务写入新数据的时候，所写的数据都会带上写入者的事务ID。表中的每一行的created_by字段，用于保存创建该行的事务ID；deleted_by初始为空，用于保存请求删除该行的事务ID，仅用于标记为删除。事后，仅当确认没有其他事务引用该删除行的时候，才执行真正的删除操作。&lt;/p&gt;
&lt;p&gt;上图中，账号1的最后一次写操作由事务ID 3完成，账号2的最后一次写操作由事务ID 5完成。事务ID 13的操作修改两个账号数据的时候，会同时设置上这一行的旧数据被事务13删除，同时新的数据由事务13创建。这样，事务12的针对账号2的读操作，返回的就是两个版本的数据：由事务5创建而由事务13删除的数据500，和由事务13创建的数据400，这样修改和未完成的读事务两者就被隔离而不会互相影响。&lt;/p&gt;
&lt;h3 id=&#34;一致性快照的可见性原则&#34;&gt;一致性快照的可见性原则&lt;/h3&gt;
&lt;p&gt;当事务读数据库时，通过事务ID可以决定哪些对象可见和不可见。要想对上层应用维护良好的快照一致性，需要静心考虑数据的可见性规则。如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;每笔事务开始时，数据库列出所有当时尚在进行中的其他事务，然后忽略这些事务完成的部分写入，即不可见。&lt;/li&gt;
&lt;li&gt;所有中止事务所做的修改全部不可见。&lt;/li&gt;
&lt;li&gt;较晚事务ID（即晚于当前事务）所做的任何修改不可见。&lt;/li&gt;
&lt;li&gt;除此之外，其他所有的写入都对应用查询可见。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;换言之，仅当以下两个条件都成立，则该数据对象对事务可见：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;事务开始的时刻，创建该对象的事务已经完成了提交。&lt;/li&gt;
&lt;li&gt;对象还没有被标记为删除，或者即使标记了，但是删除事务在当前事务开始时还没有完成提交。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;防止更新丢失&#34;&gt;防止更新丢失&lt;/h2&gt;
&lt;p&gt;更新丢失的典型场景：应用从数据库读取某些值，根据逻辑进行修改，然后写入新值（read-modify-write）。当有两个事务在同样的数据对象上执行类似操作时，由于隔离性，第二个写操作并不包括第一个事务修改后的值，最终会导致第一个事务修改后写入的值丢失。&lt;/p&gt;
&lt;p&gt;比如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;递增计数器，或更新账户余额。&lt;/li&gt;
&lt;li&gt;对某个复杂对象的一部分内容进行修改。&lt;/li&gt;
&lt;li&gt;两个用户同时编辑一个文档。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;有以下几种解决方案：&lt;/p&gt;
&lt;h3 id=&#34;原子写操作&#34;&gt;原子写操作&lt;/h3&gt;
&lt;p&gt;使用数据库提供的原子操作，比如：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;  UPDATE counters SET value = value + 1 WHERE key = &amp;#39;foo&amp;#39;;
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;显示加锁&#34;&gt;显示加锁&lt;/h3&gt;
&lt;h3 id=&#34;自动检测更新丢失&#34;&gt;自动检测更新丢失&lt;/h3&gt;
&lt;p&gt;上面的原子写操作和锁都是通过强制“读-修改-写回”操作序列化串行执行来防止丢失更新，另一种思路是先让它们并发执行，但是如果检测到更新丢失，则会终止当前事务，强制回退到安全的“读-修改-写回”方式。&lt;/p&gt;
&lt;h4 id=&#34;原子比较和设置&#34;&gt;原子比较和设置&lt;/h4&gt;
&lt;p&gt;采用类似CAS（Compare-And-Swap）方式，只有在上次读取的数据没有发生变化时才允许更新，如果发生了变化，则回退到“读-修改-写回方式”。&lt;/p&gt;
&lt;p&gt;比如为了避免两人同时编辑文档，采用以下的sql更新语句：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;  UPDATE wiki_pages SET content = &amp;#39;new content&amp;#39; where id = 1234 AND content = &amp;#39;old content&amp;#39;;
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;写倾斜和幻读&#34;&gt;写倾斜和幻读&lt;/h2&gt;
&lt;p&gt;当多个事务同时写入同一对象时引发了两种竞争条件，然而这些并不是并发写所引起的所有问题。&lt;/p&gt;
&lt;p&gt;如下图所示，开发一个医院轮班系统，在保证至少有一个医生在值班的情况下，可以申请休假，但是这还是会出现问题：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.codedump.info/media/imgs/20190403-ddia-chapter07-transaction/7-8.jpg&#34; alt=&#34;7-8&#34; title=&#34;7-8&#34;&gt;&lt;/p&gt;
&lt;p&gt;如上图中，alice和bob是两位值班医生，两人碰巧都遇到身体不适决定请假，但是几乎同一个时刻点击了调班按钮，于是发生了上图的事情。&lt;/p&gt;
&lt;p&gt;每笔事务总是首先检查是否至少有两名医生在值班。而由于数据库使用的是快照级隔离，因此两个事务的检查都返回了两名医生，这样两个事务都得以继续执行。接着两人都更新自己的值班记录离开，两个事务都成功提交，最后的结果就是没有任何一个医生在岗。&lt;/p&gt;
&lt;h3 id=&#34;定义写倾斜&#34;&gt;定义写倾斜&lt;/h3&gt;
&lt;p&gt;这种情况称为”写倾斜“，既不是脏写，也没有导致数据丢失。两次事务更新的是不同的对象（alice和bob的值班记录），写冲突并不那么直接。&lt;/p&gt;
&lt;p&gt;可以将写倾斜问题视为一种更加广泛的更新丢失问题：即如果两个事务读取相同的一组对象，然后更新其中一部分：不同的事务可能更新不同的对象，则可能发生写倾斜；而如果不同的事务更新的是同一个对象，则可能发生脏写或更新丢失（具体取决于事件窗口）。&lt;/p&gt;
&lt;h3 id=&#34;更多写倾斜例子&#34;&gt;更多写倾斜例子&lt;/h3&gt;
&lt;h3 id=&#34;为何产生写倾斜&#34;&gt;为何产生写倾斜&lt;/h3&gt;
&lt;p&gt;写倾斜都有类似的模式：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;输入一些条件，按照条件查询出满足条件的行。&lt;/li&gt;
&lt;li&gt;根据查询结果，应用层决定下一步操作。&lt;/li&gt;
&lt;li&gt;应用程序需要更新一部分数据，而这个更新操作会改变步骤2的做出决定的前提条件，即写入之后再执行步骤1的查询操作将得到不同的结果。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这种在一个事务中的写入改变了另一个事务查询结果的现象，称为幻读（phantom）。&lt;/p&gt;
&lt;h3 id=&#34;实体化冲突&#34;&gt;实体化冲突&lt;/h3&gt;
&lt;h2 id=&#34;串行化&#34;&gt;串行化&lt;/h2&gt;
&lt;h2 id=&#34;可串行化的快照隔离serializability-snapshot-isolation简称ssi&#34;&gt;可串行化的快照隔离（serializability Snapshot Isolation，简称SSI）&lt;/h2&gt;
&lt;h3 id=&#34;悲观与乐观的并发控制&#34;&gt;悲观与乐观的并发控制&lt;/h3&gt;
&lt;p&gt;两阶段加锁是典型的悲观并发控制机制。基于这样的设计原则：如果某些操作可能出错，那么直接放弃，采用等待方式直到绝对安全。这与多线程编程中的互斥锁一样。&lt;/p&gt;
&lt;p&gt;某种意义上来说，串行执行是极端悲观的选择：事务执行期间，等价于事务对整个数据库持有互斥锁。&lt;/p&gt;
&lt;p&gt;相比之下，可串行化的快照隔离则是一种乐观并发控制。在这种情况下，如果可能发生潜在冲突，事务会继续执行而不是中止，寄希望于一切相安无事；而当事务提交时，数据库会检查是否确实发生了冲突，如果是的话中止事务再进行重试。&lt;/p&gt;
&lt;h3 id=&#34;基于过期的条件做决定&#34;&gt;基于过期的条件做决定&lt;/h3&gt;
&lt;p&gt;事务是基于某些前提条件而决定采取行动，在事务开始时条件成立。&lt;/p&gt;
&lt;p&gt;那么数据库如何知道查询结果是否发生了变化？可以分为以下两种情况：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;读取是否作用于一个（即将）过期的MVCC对象。&lt;/li&gt;
&lt;li&gt;检查写入是否影响即将完成的读取。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;检测是否读取了过期的mvcc对象&#34;&gt;检测是否读取了过期的MVCC对象&lt;/h4&gt;
</description>
      
    </item>
    
    <item>
      <title>《数据密集型应用系统设计》第六章数据分区笔记</title>
      <link>https://www.codedump.info/post/20181124-ddia-chapter06-partitioning/</link>
      <pubDate>Tue, 02 Apr 2019 22:17:24 +0800</pubDate>
      
      <guid>https://www.codedump.info/post/20181124-ddia-chapter06-partitioning/</guid>
      
      <description>&lt;h1 id=&#34;键值数据的分区&#34;&gt;键值数据的分区&lt;/h1&gt;
&lt;h2 id=&#34;基于关键字区间的分区&#34;&gt;基于关键字区间的分区&lt;/h2&gt;
&lt;p&gt;给每个分区分配一段连续的关键字或者关键字区间（以最小值和最大值来指示），从关键字区间的上下限可以确定哪个分区包含这些关键字。&lt;/p&gt;
&lt;p&gt;关键字的区间段不一定要均匀分布，这是因为数据本身可能就不是均匀的。比如，某些分区包含以A和B开头字母的键，而某些分区包含了T、U、V、X、Y和Z开始的单词。&lt;/p&gt;
&lt;p&gt;基于关键字的区间分区的缺点是某些访问模式会导致热点（hot spot）。比如关键字是时间戳，分区对应一个时间范围，那么可能会出现所有的写入操作都集中在同一个分区（比如当天的分区），而其他分区始终处于空闲状态。&lt;/p&gt;
&lt;p&gt;为了避免类似的问题，需要使用时间戳以外的其他内容作为关键字的第一项。&lt;/p&gt;
&lt;h2 id=&#34;基于关键字hash值分区&#34;&gt;基于关键字Hash值分区&lt;/h2&gt;
&lt;p&gt;基于关键字Hash值分区，可以解决上面提到的数据倾斜和热点问题，但是丧失了良好的区间查询特性。&lt;/p&gt;
&lt;h2 id=&#34;负载倾斜和热点&#34;&gt;负载倾斜和热点&lt;/h2&gt;
&lt;p&gt;基于关键字Hash值分区的办法，可以减轻数据热点问题，但是不能完全避免这类问题。一种常见的极端场景是，社交网络上某个名人有几百万的粉丝，当其发布一些热点事件时可能会引起访问风暴。此时，Hash起不到任何分流的作用。&lt;/p&gt;
&lt;p&gt;大部分系统解决不了这个问题，只能通过应用层来解决这类问题。比如某个关键字被确认是热点，一个简单的技术就是在关键字的开头或结尾处添加随机数，这样将访问分配到不同的分区上。但是随之而来的问题就是，之后的任何读取都需要额外的工作，必须将这些分区上的读取数据进行合并。&lt;/p&gt;
&lt;h1 id=&#34;分区与二级索引&#34;&gt;分区与二级索引&lt;/h1&gt;
&lt;p&gt;键值类数据库的分区相对还简单一些，但是如果涉及到二级索引就变得复杂了。二级索引主要的挑战在于：它们不能规整的映射到分区中。&lt;/p&gt;
&lt;h2 id=&#34;基于文档分区的二级索引&#34;&gt;基于文档分区的二级索引&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://www.codedump.info/media/imgs/20181124-ddia-chapter06-partitioning/figure6-4.jpg&#34; alt=&#34;figure 6-4&#34; title=&#34;figure 6-4&#34;&gt;&lt;/p&gt;
&lt;p&gt;上图中，数据根据ID 进行分区，但是实际查询的时候，还可以按照颜色和厂商进行过滤，所以每个分区上面还创建了颜色和厂商的索引。每次往分区中写入新数据时，自动创建这些二级索引。&lt;/p&gt;
&lt;p&gt;在这种索引方式中，每个分区完全独立。各自维护自己的二级索引。因此文档索引也成为本地索引，而不是全局索引。&lt;/p&gt;
&lt;p&gt;但是读取的时候，需要查询所有的分区数据然后进行合并才返回给客户端，这种叫分散/聚集（scatter/gather）。&lt;/p&gt;
&lt;h2 id=&#34;基于词条的二级索引&#34;&gt;基于词条的二级索引&lt;/h2&gt;
&lt;p&gt;可以对所有的数据构建全局索引，而不是每个分区维护自己的本地索引。而且吧，为了避免成为瓶颈，不能将全局索引放在一个节点上，否则又破坏了分区均衡的目标，因此全局索引数据也需要进行分区。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.codedump.info/media/imgs/20181124-ddia-chapter06-partitioning/figure6-5.jpg&#34; alt=&#34;figure 6-5&#34; title=&#34;figure 6-5&#34;&gt;&lt;/p&gt;
&lt;p&gt;上图中，所有数据分区中的颜色进行了分区，比如从a到r开始的颜色放在了分区0中，从s到z的颜色放在了分区1中，类似的，厂商索引也被分区。这种索引方式成为词条分区（term-partitioned）。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;优点：读取高效，不需要采用scatter/gather方式对所有分区都进行查询；&lt;/li&gt;
&lt;li&gt;缺点：写入速度慢并且非常复杂，主要是因为单个文档需要更新的时候，里面可能涉及多个二级索引，而二级索引又放在不同的节点上。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在实践中，对全局二级索引数据的更新一般都是异步进行的。&lt;/p&gt;
&lt;h1 id=&#34;分区再平衡rebalancing-partitions&#34;&gt;分区再平衡（Rebalancing Partitions）&lt;/h1&gt;
&lt;p&gt;实际中，数据会发生某些变化，这时候需要将数据和请求从一个节点转移到另一个节点。这样的一个迁移负载的过程称为再平衡（rebalance）。&lt;/p&gt;
&lt;p&gt;分区再平衡至少需要满足：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;平衡之后，负载、数据存储、读写请求能够在集群范围内更均匀分布。&lt;/li&gt;
&lt;li&gt;再平衡过程中，数据库可以继续处理客户端的读写请求。&lt;/li&gt;
&lt;li&gt;避免不必要的负载迁移。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面谈各种再平衡策略。&lt;/p&gt;
&lt;h2 id=&#34;为什么不能用取模&#34;&gt;为什么不能用取模？&lt;/h2&gt;
&lt;p&gt;对节点数进行取模的方式，最大的问题在于如果节点的数据发生了变化，会导致很多关键字从现有的节点迁移到另一个节点。&lt;/p&gt;
&lt;h2 id=&#34;固定数量的分区&#34;&gt;固定数量的分区&lt;/h2&gt;
&lt;p&gt;创建远超实际节点数的分区数，然后给每个节点分配多个分区。比如只有10个节点的集群，划分了1000个逻辑分区。&lt;/p&gt;
&lt;p&gt;如果集群中添加了一个新节点，该新节点就可以从每个现有节点上匀走几个分区，直到分区再次达到全局平衡。&lt;/p&gt;
&lt;p&gt;这个方式的优点在于，关键字与逻辑分区的映射关系一开始就固定下来了，节点数量的变更只是改变了逻辑分区分布在哪些节点上。节点间迁移分区数据需要时间，这个过程中，就分区依然可以处理客户端的读写请求。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.codedump.info/media/imgs/20181124-ddia-chapter06-partitioning/figure6-6.jpg&#34; alt=&#34;figure 6-6&#34; title=&#34;figure 6-6&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;动态分区&#34;&gt;动态分区&lt;/h2&gt;
&lt;h2 id=&#34;按节点比例分区&#34;&gt;按节点比例分区&lt;/h2&gt;
&lt;h1 id=&#34;自动与手动再平衡操作&#34;&gt;自动与手动再平衡操作&lt;/h1&gt;
&lt;h1 id=&#34;请求路由&#34;&gt;请求路由&lt;/h1&gt;
&lt;p&gt;当客户端需要发起请求时，如果知道应该连接哪个节点？如果发生了分区再平衡，分区与节点的对应关系发生了变化。&lt;/p&gt;
&lt;p&gt;这类问题属于典型的服务发现（service discover）问题。服务发现问题不限于数据库中，任何需要通过网络访问的系统都有这样的需求，尤其是服务目标需要支持高可用时。&lt;/p&gt;
&lt;p&gt;一般有以下三种处理策略。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;客户端可以发送请求给任意节点，如果节点不能处理请求则路由到可以处理该请求的分区，应答之后再回复客户端。&lt;/li&gt;
&lt;li&gt;所有客户端的请求发送到一个路由层，由该路由层决定请求应该转发到哪个分区。&lt;/li&gt;
&lt;li&gt;客户端需要感知分区与节点之间的映射关系。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;但是不管是上面的哪种方案，核心问题都是：做为路由决策的组件，如何知道分区与节点的对应关系以及其变化情况？&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.codedump.info/media/imgs/20181124-ddia-chapter06-partitioning/figure6-7.jpg&#34; alt=&#34;figure 6-7&#34; title=&#34;figure 6-7&#34;&gt;&lt;/p&gt;
&lt;p&gt;很多分布式系统依赖于独立的协调服务（比如zookeeper、etcd等）跟踪集群范围内的元数据（metadata）。如下图所示，每个节点都像zookeeper注册自己，zookeeper维护了分区到节点的最终映射关系。而其他参与者（比如路由层或者分区感知的客户端）向zookeeper订阅此信息。当分区信息发生了增删时，zookeeper会主动通知，这样就能够保持最新的状态。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.codedump.info/media/imgs/20181124-ddia-chapter06-partitioning/figure6-8.jpg&#34; alt=&#34;figure 6-8&#34; title=&#34;figure 6-8&#34;&gt;&lt;/p&gt;
&lt;p&gt;还有另一种策略，在节点之间使用gossip协议来同步集群状态的变化，请求可以发送到任何节点，由该节点负责将其转发到正确的节点上。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>《数据密集型应用系统设计》第五章数据复制笔记</title>
      <link>https://www.codedump.info/post/20181118-ddia-chapter05-replication/</link>
      <pubDate>Mon, 01 Apr 2019 18:19:22 +0800</pubDate>
      
      <guid>https://www.codedump.info/post/20181118-ddia-chapter05-replication/</guid>
      
      <description>&lt;h1 id=&#34;主从复制&#34;&gt;主从复制&lt;/h1&gt;
&lt;p&gt;集群中有一个主节点，写操作都必须经过主节点完成，读操作主从节点都可以处理。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.codedump.info/media/imgs/20181118-ddia-chapter05-replication/figure5-1.jpg&#34; alt=&#34;figure 5-1&#34; title=&#34;figure 5-1&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;同步复制和异步复制&#34;&gt;同步复制和异步复制&lt;/h2&gt;
&lt;h3 id=&#34;同步复制&#34;&gt;同步复制&lt;/h3&gt;
&lt;p&gt;数据在副本上落盘才返回。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;优点：保证在副本上的数据是最新数据。&lt;/li&gt;
&lt;li&gt;缺点：延迟高，响应慢。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;异步复制&#34;&gt;异步复制&lt;/h3&gt;
&lt;p&gt;数据不保证在副本上落盘。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;优点：延迟低&lt;/li&gt;
&lt;li&gt;不能保证在副本上的数据最新。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;不能把集群中所有节点设置为同步节点，因为这样的话任何一个节点的停滞都会导致整个集群的不可用。像Paxos、Raft算法，都要求集群中大多数节点返回就可以了。部分同步、部分异步的集群配置成为半同步（semi-sync）的集群配置。&lt;/p&gt;
&lt;h2 id=&#34;新增新的从节点&#34;&gt;新增新的从节点&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;主节点生成快照数据&lt;/li&gt;
&lt;li&gt;主节点将快照数据发送到从节点。&lt;/li&gt;
&lt;li&gt;从节点请求主节点快照数据之后的数据。&lt;/li&gt;
&lt;li&gt;重复上面三步直到从节点追上主节点的进度。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;处理节点失效&#34;&gt;处理节点失效&lt;/h2&gt;
&lt;h3 id=&#34;从节点失效&#34;&gt;从节点失效&lt;/h3&gt;
&lt;p&gt;从节点崩溃恢复之后按照前面新增新的从节点的步骤来追上主节点的数据进度。&lt;/p&gt;
&lt;h3 id=&#34;主节点失效&#34;&gt;主节点失效&lt;/h3&gt;
&lt;p&gt;主节点失败时需要提升某个从节点为新的主节点，同时需要通知客户端新的主节点。&lt;/p&gt;
&lt;p&gt;自动切换主节点的步骤通常如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;确认主节点失效。大部分系统采用基于超时的机制，主从节点直接发送心跳消息，主节点在某个时间内都没有响应，则认为主节点已经失效。&lt;/li&gt;
&lt;li&gt;选举新的主节点。通过选举的方式（超过半数以上的从节点达成共识）来选举新的主节点，新的主节点是与旧的主节点数据差异最小的一个，最小化数据丢失的风险。&lt;/li&gt;
&lt;li&gt;重新配置使新的主节点上线。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;除了以上步骤之外，还有以下问题需要考虑：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;如果使用异步复制机制，而且在失效之前，新的主节点并没有收到旧的主节点的所有数据，那么在旧的主节点重新上线之后，未完成复制的数据将被丢弃。&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;可能会出现集群同时存在两个主节点的情况，也就是所谓的脑裂（split brain）现象，此时两个主节点都认为自己是主节点并且都能接收客户端的写数据请求，会导致数据丢失或者破坏。&lt;/li&gt;
&lt;li&gt;如何设置合理的超时时间来判断主节点失效？如果太大意味着总体恢复时间长，如果太小意味着某些情况下可能主节点并未失效但是被误判为失效了，比如网络峰值导致延迟高等原因，这样会导致很多不必要的主节点切换。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;上述的问题，包括节点失效、网络不可靠、副本一致性、持久性、可用性与延迟之间的各种细微的权衡，正是分布式系统核心的基本问题。&lt;/p&gt;
&lt;h2 id=&#34;复制日志的实现&#34;&gt;复制日志的实现&lt;/h2&gt;
&lt;h3 id=&#34;基于语句的复制&#34;&gt;基于语句的复制&lt;/h3&gt;
&lt;p&gt;主节点记录所执行的每个写请求并将该语句做为日志发送给从节点。但是有些场景并不适合这么做，比如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;调用任何非确定函数的语句，比如NOW()获得当前时间，RAND()返回一个随机数。&lt;/li&gt;
&lt;li&gt;语句中使用了自增列，或者依赖于当前数据库的数据。&lt;/li&gt;
&lt;li&gt;有副作用的语句，在每个副本上面执行的效果不一样。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;基于预写日志wal&#34;&gt;基于预写日志(WAL)&lt;/h3&gt;
&lt;p&gt;将对数据库的操作写入日志，传送到从节点上然后执行，得到与主节点相同的数据副本。&lt;/p&gt;
&lt;h3 id=&#34;基于行的逻辑日志复制&#34;&gt;基于行的逻辑日志复制&lt;/h3&gt;
&lt;p&gt;所谓的逻辑日志，就是复制与存储引擎采用不同的日志格式，这样复制与存储逻辑剥离，这种日志称为逻辑日志，与物理存储引擎的数据区分开。由于逻辑日志与存储引擎逻辑上解耦，因此可以更好的向后兼容，也更好的能被外部程序解析。&lt;/p&gt;
&lt;p&gt;对于关系型数据库，其逻辑日志是一系列用来描述数据表行级别的写请求：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;插入行：日志包括所有相关列的新值。&lt;/li&gt;
&lt;li&gt;删除行：日志中保证要有足够的信息来唯一标识待删除的行，通常是主键。&lt;/li&gt;
&lt;li&gt;更新行：日志中保证要有足够的信息来唯一标识待更新的行，同时也有所有列的新值。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;复制滞后replication-lag问题&#34;&gt;复制滞后（replication lag）问题&lt;/h1&gt;
&lt;p&gt;如果一个应用正好从一个异步复制的从节点上读取数据，则可能读取不到最新的数据，这是因为主从节点的数据不一致导致的。理论上不一致状态在时间上并没有上限。以下描述几个复制滞后导致的问题。&lt;/p&gt;
&lt;h2 id=&#34;读自己的写reading-your-own-writes&#34;&gt;读自己的写（reading your own writes）&lt;/h2&gt;
&lt;p&gt;用户在写入数据不久就马上查看数据，而新数据并未到达从节点，这样在用户看来可能读到了旧的数据。这样情况需要“写后读一致性（read-after-write consistency）”，该机制保证每次用户读到的都是自己最近的更新数据，但是对其他用户则没有任何保证。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.codedump.info/media/imgs/20181118-ddia-chapter05-replication/figure5-3.jpg&#34; alt=&#34;figure 5-3&#34; title=&#34;figure 5-3&#34;&gt;&lt;/p&gt;
&lt;p&gt;在上图中，用户1234首先向主节点写入数据，SQL执行成功之后返回，而此时用户再次向从节点2发起读刚才写入数据的请求，但是却读到了旧的数据。&lt;/p&gt;
&lt;p&gt;有以下方案实现写后读一致性。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果用户访问可能会被修改的内容，从主节点读取。比如社交网络的本用户首页信息只会被本人修改，访问用户自己的首页信息通过主节点，而访问其他用户的首页信息则走的从节点。&lt;/li&gt;
&lt;li&gt;如果应用大部分内容都可能被所有用户修改，则上述方法不太适用。此时需要其他机制来判断哪些请求需要走主节点，比如更新后一分钟之内的请求都走的主节点。&lt;/li&gt;
&lt;li&gt;客户端可以记住自己最近更新数据的时间戳，在请求数据时带上时间戳，如果副本上没有至少包含该时间戳的数据则转发给其他副本处理，直到能处理为止。但是在这里，“时间戳”可以是逻辑时钟（比如用来指示写入数据的日志序列号）或者实际系统时钟（而使用系统时间又将时间同步变成了一个关键点）。&lt;/li&gt;
&lt;li&gt;如果副本分布在多数据中心，必须将请求路由到主节点所在的数据中心。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;单调读monotonic-reads&#34;&gt;单调读（monotonic reads）&lt;/h2&gt;
&lt;p&gt;单调读一致性保证不会发生多次读同一条数据出现回滚（moving backward）的现象。这个是比强一致性弱，但是比最终一致性强的保证。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.codedump.info/media/imgs/20181118-ddia-chapter05-replication/figure5-4.jpg&#34; alt=&#34;figure 5-4&#34; title=&#34;figure 5-4&#34;&gt;&lt;/p&gt;
&lt;p&gt;在上图中，用户2345发起了两次读请求，第一次向从节点1发起的请求拿到了最新的数据，但是第二次向从节点2发起的请求得到了旧的数据，这在用户看来，数据发生了“回滚”。&lt;/p&gt;
&lt;p&gt;单调读一致性可以确保不会发生这种异常。当读取数据时，单调读保证：如果某个用户进行多次读取，则绝对不会看到数据回滚现象，即在读取到新值之后又发生读取到旧值的情况。&lt;/p&gt;
&lt;p&gt;实现单调读一致性的一种方式每个用户的每次读取都从固定的同一副本上进行读取。&lt;/p&gt;
&lt;h2 id=&#34;前缀一致读consistent-prefix-reads&#34;&gt;前缀一致读（consistent prefix reads）&lt;/h2&gt;
&lt;p&gt;前缀一致性读保证，对于一系列按照某个顺序发生的写请求，读取这些内容时也会按照当时写入的顺序来。&lt;/p&gt;
&lt;p&gt;例如，正常情况下，是如下的对话：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;poons先生：cake小姐，您能看见多远的未来？&lt;/li&gt;
&lt;li&gt;cacke小姐：通常约10秒，poons先生。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://www.codedump.info/media/imgs/20181118-ddia-chapter05-replication/figure5-5.jpg&#34; alt=&#34;figure 5-5&#34; title=&#34;figure 5-5&#34;&gt;&lt;/p&gt;
&lt;p&gt;但是在上图中，从观察者角度，数据的先后顺序发生了混淆，导致了逻辑上的混乱。&lt;/p&gt;
&lt;p&gt;这种问题是分区情况下出现的特殊问题，在分布式数据库中，不同的分区独立运行，因此不存在全局写入顺序，这就导致用户从数据库中读取数据时，可能看到数据库某部分的旧值和一部分的新值。&lt;/p&gt;
&lt;p&gt;实现前缀一致性的一种方案是确保任何具有因果顺序关系的写入都交给一个分区来完成，但是该方案真实实现起来效率不高。&lt;/p&gt;
&lt;h2 id=&#34;复制滞后的解决方案&#34;&gt;复制滞后的解决方案&lt;/h2&gt;
&lt;h1 id=&#34;多主节点复制&#34;&gt;多主节点复制&lt;/h1&gt;
&lt;h2 id=&#34;适用场景&#34;&gt;适用场景&lt;/h2&gt;
&lt;h3 id=&#34;多数据中心&#34;&gt;多数据中心&lt;/h3&gt;
&lt;p&gt;为了容忍整个数据中心级别故障或更接近用户，可以把数据库的副本横跨多个数据中心。在每个数据中心内，采用常规的主从复制方案；而在数据中心之间，由各个数据中心的主节点来负责同其他数据中心的主节点进行数据的交换、更新。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.codedump.info/media/imgs/20181118-ddia-chapter05-replication/figure5-6.jpg&#34; alt=&#34;figure 5-6&#34; title=&#34;figure 5-6&#34;&gt;&lt;/p&gt;
&lt;p&gt;主从复制的优缺点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;优点&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;性能：每个写操作可以在本地数据中心就近快速响应，采用异步复制方式将变化同步到其他数据中心。&lt;/li&gt;
&lt;li&gt;容忍数据中心失败：单个数据中心失败，不影响其他数据中心的继续运行。&lt;/li&gt;
&lt;li&gt;容忍网络问题：主从复制模型中写操作是同步操作，对数据中心之间的网络性能和稳定性等要求更高。多主节点模型采用异步复制，可以更好的容忍这类问题。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;缺点：多个数据中心可能同时修改同一份数据，造成写冲突。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;离线客户端操作&#34;&gt;离线客户端操作&lt;/h3&gt;
&lt;p&gt;每个客户端可以认为是一个独立的数据中心，这样用户就可以在离线的状态下使用客户端，而在网络恢复之后再将数据同步到服务器。&lt;/p&gt;
&lt;h3 id=&#34;协作编辑&#34;&gt;协作编辑&lt;/h3&gt;
&lt;p&gt;允许多个用户同时编辑文档，如google docs。这样每个用户就是一个独立的数据中心了。&lt;/p&gt;
&lt;h2 id=&#34;处理写冲突&#34;&gt;处理写冲突&lt;/h2&gt;
&lt;p&gt;多主复制最大的问题就是要解决写冲突，如下图所示。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.codedump.info/media/imgs/20181118-ddia-chapter05-replication/figure5-7.jpg&#34; alt=&#34;figure 5-7&#34; title=&#34;figure 5-7&#34;&gt;&lt;/p&gt;
&lt;p&gt;两个用户同时编辑wiki页面，发生了写冲突。&lt;/p&gt;
&lt;h3 id=&#34;同步与异步冲突检测&#34;&gt;同步与异步冲突检测&lt;/h3&gt;
&lt;p&gt;如果是主从复制数据库，第二个写请求会被阻塞到第一个写请求完成。而在多主从复制模型下，两个写请求都是成功的，并且只有在之后才能检测到写冲突，而那时候要用户来解决冲突已经为时已晚了。&lt;/p&gt;
&lt;p&gt;如果要多主从复制模型来做到同步检测冲突，又失去了多主节点的优势：允许每个主节点接受写请求。&lt;/p&gt;
&lt;p&gt;因此如果确实想要做到同步检测写冲突，应该考虑使用单主节点的模型而不是多主从节点模型。&lt;/p&gt;
&lt;h3 id=&#34;避免冲突&#34;&gt;避免冲突&lt;/h3&gt;
&lt;p&gt;如果应用层能保证针对特定的一条记录，每次修改都经过同一个主节点，就能避免写冲突问题。&lt;/p&gt;
&lt;p&gt;但是，在数据中心发生故障，不得不路由请求到另外的数据中心，或者用户漫游到了另一个位置，更靠近另一个数据中心等场景下，冲突避免不再有效。&lt;/p&gt;
&lt;h3 id=&#34;收敛于一致状态&#34;&gt;收敛于一致状态&lt;/h3&gt;
&lt;p&gt;有以下方式解决冲突的收敛：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;给每个写入分配唯一的ID，如时间戳、足够长的随机数、UUID等，规定只有高ID的写入做为胜利者。如果是基于时间戳的对比，这种技术被称为后写入者获胜（last write win），但是很容易造成数据丢失。&lt;/li&gt;
&lt;li&gt;给每个副本分配一个唯一的ID，并制定规则比如最高ID的副本写入成功，这种方式也会导致数据丢失。&lt;/li&gt;
&lt;li&gt;以某种方式将这些值合并在一起。&lt;/li&gt;
&lt;li&gt;使用预定义的格式将这些冲突的值返回给应用层，由应用层来解决。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;自定义冲突解决逻辑&#34;&gt;自定义冲突解决逻辑&lt;/h3&gt;
&lt;p&gt;解决冲突最合适的方式还是依靠应用层，可以在写入或者读取时执行。&lt;/p&gt;
&lt;h2 id=&#34;拓扑结构&#34;&gt;拓扑结构&lt;/h2&gt;
&lt;h1 id=&#34;无主节点复制&#34;&gt;无主节点复制&lt;/h1&gt;
&lt;p&gt;放弃主节点，允许所有节点处理来自客户端的写请求，如Dynamo、Riak等。&lt;/p&gt;
&lt;h2 id=&#34;节点失效时写入数据库&#34;&gt;节点失效时写入数据库&lt;/h2&gt;
&lt;p&gt;对于无主节点复制的集群而言，当向有三个副本的集群写入数据时，只要其中有两个副本写入完成则认为写入成功，而可以容忍其中一个节点的失效。那么当这个失效节点重新上线时，则可能读到已经过期的数据。为了解决这个问题，当客户端从集群中读取数据时，并不是只向一个副本发起请求，而是并行地发送到多个副本，客户端可以根据数据版本号来确定哪个数据最新。&lt;/p&gt;
&lt;h2 id=&#34;读写quorum&#34;&gt;读写quorum&lt;/h2&gt;
&lt;p&gt;在有三个副本的情况下，如果有两个副本写入成功，那么意味着最多有一个副本可能包含旧的值，此时如果向至少两个副本发起读请求，通过版本号可以确定至少有一个包含新的值。&lt;/p&gt;
&lt;p&gt;推而广之，如果有n个副本，写入需要w个节点确认，读取必须至少查询r个节点，则只需要w+r&amp;gt;n，那么读取的节点中一定包含新值。&lt;/p&gt;
&lt;p&gt;上述参数通常是可以配置，比如n取奇数值，而r、w去(n+1)/2。也可以根据业务需求灵活配置，比如对于读多写少的业务，设置w=n或者r=1，这样读取速度更快，但是只要一个节点的写入失败而导致quorum写入失败。&lt;/p&gt;
&lt;h2 id=&#34;quorum一致性的局限性&#34;&gt;quorum一致性的局限性&lt;/h2&gt;
&lt;h2 id=&#34;检测并发写&#34;&gt;检测并发写&lt;/h2&gt;
&lt;p&gt;请求在不同节点上可能会呈现出不同的顺序，如下图所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.codedump.info/media/imgs/20181118-ddia-chapter05-replication/figure5-12.jpg&#34; alt=&#34;figure 5-12&#34; title=&#34;figure 5-12&#34;&gt;&lt;/p&gt;
&lt;p&gt;客户端A和B同时发起向主键X的写请求：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;节点1收到客户端A的写请求，但是由于节点紧接着就失效了，没有收到客户端B的写请求。&lt;/li&gt;
&lt;li&gt;节点2首先收到A的写请求，接着才收到B的写请求。&lt;/li&gt;
&lt;li&gt;节点3与2相反，先收到B再收到A的写请求。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果处理方式仅仅是每次收到新的写请求就简单覆盖原来的值，那么这些节点永远也无法达成一致。&lt;/p&gt;
&lt;p&gt;所以需要一种更合理的方式来解决并发写冲突。&lt;/p&gt;
&lt;h3 id=&#34;最后写入者胜出last-write-win简称lww&#34;&gt;最后写入者胜出（Last Write Win，简称LWW）&lt;/h3&gt;
&lt;p&gt;为每个写请求附件一个时间戳，然后选择最新即最大的时间戳，丢弃较早时间戳的写入，这种方案称为Last Write Win。&lt;/p&gt;
&lt;p&gt;这种方案的问题在于：物理时间本身就不可信任，一个机器上的时间到了另一个机器上并不就精准。另外，牺牲了部分的写入数据，比如某客户端写入时返回成功，但是会被后面并发写入但是被认为更晚时间的写入给覆盖掉，这样这部分认为写入成功的数据就丢失了。&lt;/p&gt;
&lt;p&gt;要确保LWW无副作用的唯一办法是：之写入一次然后写入值视为不可变。这样就能避免对同一个主键的覆盖。例如，cassandra的推荐使用方式是使用UUID做为主键，这样每个写操作都针对不同的、系统唯一的主键。&lt;/p&gt;
&lt;h3 id=&#34;happen-before关系与并发&#34;&gt;Happen-before关系与并发&lt;/h3&gt;
&lt;p&gt;两件事情A、B只有三种可能存在的关系：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A在B之前发生。&lt;/li&gt;
&lt;li&gt;B在A之前发生。&lt;/li&gt;
&lt;li&gt;A、B并发进行。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一件事情在另一件事情之前发生，说明两者之间存在依赖关系。比如A说了一句话，而B需要对这句话进行回应，回应这个事件就依赖于A说话这个事件，此时B的回应依赖于A的话，因此B的回应肯定发生A说话之后。&lt;/p&gt;
&lt;p&gt;如果两件事情之间没有依赖关系，那么先后顺序是无所谓的，即并发进行。&lt;/p&gt;
&lt;p&gt;来看一个实际的例子，两个客户端同时向一个购物车添加物品：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.codedump.info/media/imgs/20181118-ddia-chapter05-replication/figure5-13.jpg&#34; alt=&#34;figure 5-13&#34; title=&#34;figure 5-13&#34;&gt;&lt;/p&gt;
&lt;p&gt;流程如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;客户端1首先将牛奶放入购物车，服务器分配版本号1，将值与版本号一起返回给客户端。&lt;/li&gt;
&lt;li&gt;客户端2将鸡蛋放入购物车，因为此时客户端2并不知道客户端1已经放入了牛奶，因此认为鸡蛋是购物车中的唯一物品。服务器写入并分配版本号2，将鸡蛋和牛奶存储为两个单独的值，最后将版本号2和值返回给客户端2。&lt;/li&gt;
&lt;li&gt;同理，客户端1再次写入时也没有意识到2已经修改了购物车，此时它想继续添加免费，认为此时购物车的内容应该是[牛奶,面粉]，因此将这个值与版本号1一起发给服务器。服务器收到之后，意识到是针对版本号1做的修改，即将[牛奶]修改成[牛奶面粉]，但是另一个值[鸡蛋]则是新的并发操作。因此，服务器分配了一个新的版本号3，版本号3的值[牛奶,面粉]覆盖版本1的[牛奶]，同时保留版本号2的值[鸡蛋]，一起返回给客户端1。&lt;/li&gt;
&lt;li&gt;客户端2想加入火腿，而它也不知道客户端1添加了面粉。其收到的最后一个响应中服务器给的值是[牛奶]和[鸡蛋]，因此进行了合并并且加入自己要添加的火腿，向服务器发送了版本号2以及新的值[鸡蛋,牛奶,火腿]。服务器检测到版本号2将覆盖原来的值[鸡蛋]，但是与[牛奶,面粉]是同时发生，所以设置了版本号4并将这些值一起返回给客户端2。&lt;/li&gt;
&lt;li&gt;最后，客户端1想添加培根，在以前的版本3中从服务器收到了值[牛奶,面粉]和[鸡蛋]，所以进行了合并，将添加了培根以及合并之后的值[牛奶,面粉,鸡蛋,培根]和版本号3来覆盖[牛奶,面粉]，但是由于与[鸡蛋，牛奶，火腿]并发，所以服务器会保留这些值。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;服务器判断操作是否并发的依据主要依靠版本号：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;服务器为每个主键维护一个版本号，每当主键新值写入时递增版本号，并将新版本号与写入的值一起保存。&lt;/li&gt;
&lt;li&gt;当客户端读取主键时，服务器将返回所有（未被覆盖）当前值以及最新的版本号，且要求写入之前，客户端必须先发送读请求。&lt;/li&gt;
&lt;li&gt;客户端写主键，写请求必须包含之前读到的版本号、读到的值和新值合并后的集合。写请求的响应可以像读请求一样，返回所有值，这样就可以像购物车例子那样一步步链接起多个写入的值。&lt;/li&gt;
&lt;li&gt;当服务器收到带有特定版本的写入时，覆盖该版本号或更低版本的所有值（因为知道这些值已经被合并到新传入的值集合中），但必须保留更高版本号的所有值（因为这些值与当前的写操作属于并发）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这种方案不会让写入的值丢失，但是需要在客户端做合并操作，将多个写入的值进行合并。&lt;/p&gt;
</description>
      
    </item>
    
  </channel>
</rss>
